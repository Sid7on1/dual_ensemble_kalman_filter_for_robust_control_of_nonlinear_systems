{
  "agent_id": "coder2",
  "task_id": "task_2",
  "files": [
    {
      "filename": "pde_simulator.py",
      "purpose": "Implementation of the PDE simulator using controlgym",
      "priority": "medium",
      "dependencies": [
        "controlgym"
      ],
      "key_functions": [
        "simulate_pde",
        "get_pde_state"
      ],
      "estimated_lines": 200,
      "complexity": "medium"
    }
  ],
  "project_info": {
    "project_name": "Dual Ensemble Kalman Filter for Robust Control of Nonlinear Systems",
    "project_type": "optimization",
    "description": "Implementation of a dual ensemble Kalman filter approach for robust control of nonlinear systems, with application to partial differential equations (PDEs)",
    "key_algorithms": [
      "Dual Ensemble Kalman Filter (dual EnKF)",
      "Lyapunov redesign",
      "Dynamic Mode Decomposition with control (DMDc)"
    ],
    "main_libraries": [
      "numpy",
      "scipy",
      "controlgym"
    ]
  },
  "paper_content": "PDF: cs.SY_2508.21684v1_A-Dual-Ensemble-Kalman-Filter-Approach-to-Robust-C.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nA Dual Ensemble Kalman Filter Approach to Robust Control of\nNonlinear Systems: An Application to Partial Differential Equations\nAnant A. Joshi, Saviz Mowlavi, Mouhacine Benosman\nAbstract \u2014 This paper considers the problem of data-driven\nrobust control design for nonlinear systems, for instance,\nobtained when discretizing nonlinear partial differential equa-\ntions (PDEs). A robust learning control approach is developed\nfor nonlinear affine in control systems based on Lyapunov\nredesign technique. The robust control is developed as a sum\nof an optimal learning control which stabilizes the system\nin absence of disturbances, and an additive Lyapunov-based\nrobustification term which handles the effects of disturbances.\nThe dual ensemble Kalman filter (dual EnKF) algorithm\nis utilized in the optimal control design methodology. A\nsimulation study is done on the heat equation and Burgers\npartial differential equation.\nI. I NTRODUCTION\nIn this paper, we are primarily interested in the robust\ncontrol of nonlinear affine in control systems modelled as\n\u02d9x(t) =a(x(t)) +b(x(t))u(t) +d(t, x), x(0) = x0(1)\nwhere x(t)\u2208Rn,u(t)\u2208Rmandd(t, x)\u2208Rlfor all (t, x),\nanddis regarded as an unknown disturbance bounded in\nnorm by a known real valued function \u03bb(t, x). The emphasis\nis on systems where we do not have explicit access to\naandb, but we can access trajectories of the system,\neither generated by a simulator or collected from real-life\nexperiments. For simplicity, we focus in the remaining of\nthe paper on the case of simulated trajectories. Furthermore,\nwe target the application of our algorithms to the control of\nPDEs, when their discretized model (1) is available as a\nsimulator.\nIndeed, PDE control is challenging because a PDE is by\nnature infinite dimensional and may have strong nonlinear-\nities. The reduce-then-design approach is a well researched\ncontrol methoology, which involves discretization of the\nPDE followed by dimensionality reduction, which yields a\nmodel amenable to the application of standard model based\ncontrol approaches [20], [14], [2], [25], [28]. There is a\nrecent body of work using data driven methods for building\nmore accurate reduced order models of the PDE [3], [11],\n[16]. While more accurate reduced order models are bene-\nficial to the performance of the controller, their complexity\nmakes them computationally challenging to implement.\nA. A. Joshi (anantaj2@illinois.edu) is with the Coordinated Science\nLaboratory and the Department of Mechanical Science and Engineering at\nthe University of Illinois Urbana-Champaign, Urbana, IL 61801, USA. S.\nMowlavi (mowlavi@merl.com) is with Mitsubishi Electric Research Lab-\noratories (MERL), Cambridge, MA 02139, USA. Mouhacine Benosman\n(mbenosman@ieee.org) is with Amazon Robotics, North Reading, MA\n01864, USA. This work was completed during A. A. Joshi\u2019s internship at\nMERL and prior to M. Benosman joining Amazon Robotics.In this work, we focus more on the design of the con-\ntroller with emphasis on obtaining the control policy with\nonly simulator for the model, without access to the actual\nmodel parameters. See for example [6], [5], [8], [12] for\nsome other recent efforts in obtaining data-driven controllers\nusing a simulator of the PDE. In particular, we work on\nthe stabilization problem, which aims to drive the PDE\nstate to zero, by posing it as an optimal control problem.\nThe first step is to discretize the PDE in space, to yield a\nhigh-dimensional nonlinear system of ordinary differential\nequations (ODEs). We consider two different cases for the\nsimulator availability:\n1) a linear reduced order simulator, obtained using Dy-\nnamic Mode Decomposition with control (DMDc)\n[24], [32] is available.\n2) an extension to the case where a simulator for the full\nhigh-dimensional nonlinear discretized model of the\nPDE is available.\nWe build on previous work in this area [32] by taking\na robust learning control approach. We construct the robust\ncontrol as a sum of two parts: an optimal learning controller,\nwhich we expect to produce stabilization in the absence\nof disturbances, and an additional term that builds on\nthe optimal control term and is based on the Lyapunov\nredesign approach [18] to suppress the effect caused by\ndisturbances. The optimal control is approximated using the\ndual ensemble Kalman filter (dual EnKF) algorithm [15],\nwhich simulates multiple interacting copies of the system.\nThe ensemble Kalman filter (EnKF) [30], [26] has his-\ntorically been an algorithm used for filtering, and is a key\nnumerical method especially for high-dimensional systems,\nfor example in weather prediction [7] (see [4] for more\nreferences). It features the design of an interacting particle\nsystem to sample from the posterior density of a filter-\ning problem to provide a state estimate. The dual EnKF\nalgorithm, inspired from the EnKF, computes the optimal\ncontrol by converting the control problem into the prob-\nlem of sampling from an appropriate probability density,\nthrough the log-transform duality between optimal control\nand filtering [10]. Such an approach for solving optimal\ncontrol by posing it as a sampling problem is well studied\n(see [27], [17], [29], [13], [21]). The novelty of the dual\nEnKF lies in the design of an interacting particle system,\ninspired from the EnKF, to solve the sampling problem.\nThe dual EnKF-based control exhibits a distinct advantage\nin its performance on systems of high dimension \u2013 a traitarXiv:2508.21684v1  [math.OC]  29 Aug 2025\n\n--- Page 2 ---\ninherited from the EnKF. It performs almost two orders of\nmagnitude faster when compared with policy gradient type\napproaches [23], [9] as was computationally demonstrated\nin [15].\nThe paper is organized as follows. In Section II we\nintroduce and solve the problem for the case when the\nsystem is linear time invariant (LTI) to clarify the main\nideas. Then we present the extension to the nonlinear affine\nin control case in Section III. Finally, in Section IV we\npresent an application to the stabilization of PDEs, with the\nheat equation and Burgers\u2019 equation as examples.\nNotation: |\u00b7|denotes the Euclidean norm,\u22a4denotes the\ntranspose of a matrix, and Irefers to the identity matrix.\nII. S OLUTION FOR LINEAR REDUCED ORDER\nSIMULATOR\nConsider the simplified case when (1) is a linear time\ninvariant (LTI) system:\n\u02d9x(t) =Ax(t) +Bu(t) +d(t, x) (2)\nwhere as earlier, x(t)\u2208Rn,u(t)\u2208Rmandd(t, x)\u2208Rlfor\nall(t, x), and dis regarded as a disturbance. From here on\nwe may suppress the targument to improve readability. We\nconstruct the robust control as a sum of a stabilizing control\nand an additional robustification term. We first present a\nrecipe to obtain the stailizing control, and then to obtain\nthe robust control, and lastly a simulator based method to\nobtain the two.\nA. Stabilizing control\nConsider, for any arbitrary but fixed T > 0, the optimal\ncontrol problem\nmin\nu(\u00b7)\u0012\nx(T)\u22a4Gx(T) +1\n2ZT\n0|Cx(t)|2+u(t)\u22a4Ru(t)| {z }\n=:L(x(t),u(t))dt\u0013\n(3a)\ns.t. system (2) with zero disturbance, that is, d\u22610(3b)\nwhere C\u2208Rn1\u00d7n(for arbitrary n1>0),G\u2208Rn\u00d7nand\nR\u2208Rm\u00d7m, and define Q:=C\u22a4C.\nAssumption 1: We make the following assumptions\nabout the structure of the optimal control problem (3)\n(i)(A, B)is controllable and (A, C)is observable\n(ii)R, G\u227b0\nThen there exists a positive definite solution {P(t) :t\u2208\n[0, T]}to the differential Riccati equation (DRE) [19, Chap-\nter 3]:\n\u2212\u02d9P=A\u22a4P+PA\u2212PBR\u22121B\u22a4P+Q, P T=G\nwhich converges to \u00afP\u227b0asT\u2192 \u221e and\u00afPsolves the\nalgebraic Riccati equation (ARE)\n0 =A\u22a4\u00afP+\u00afPA\u2212\u00afPBR\u22121B\u22a4\u00afP+Q.\nMoreover, the control u=\u2212\u00afKx with \u00afK:=R\u22121B\u22a4\u00afP\nmakes the system asymptotically stable [19, Theorem 3.7].B. Robust control\nSuppose that for the system (2) in the case of no dis-\nturbance, there exists a stabilizing control u=\u2212\u00afKxand a\nstrictly positive definite Lyapunov function V(x) =1\n2x\u22a4\u00afPx\nwith \u02d9V=\u2212x\u22a4\u00afP(A\u2212B\u00afK)x\u22640along the controlled\ntrajectories with zero disturbance. We let \u00afKbe the gain\nobtained from the optimal control demonstrated previously\nand\u00afPbe the solution of the ARE.\nWe are interested in the idea of disturbance rejection\nusing Lyapunov redesign [18, Chapter 14]. The idea is to\ndesign a robust control udsuch that the controller u=\n\u2212\u00afKx+udmakes the system (1) asymptotically stable in the\npresence of disturbance. To that end, we make the following\nassumption, and then present a design methodology for ud.\nAssumption 2: (i) The rank of Bisn.\n(ii) There exists a known \u03bbsuch that 0\u2264 |d(t, x)|<\n\u03bb(t, x)<\u221efor each (t, x)\u2208[0,\u221e)\u00d7Rn.\nRemark 1: Assumption 2-(i) is required to motivate the\ntheoretical derivation for the linear system, but we relax it\nin our implementations for PDE control.\nConsider the controller u=\u2212\u00afKx+udwith\nud:=\u2212\u03bb(t, x)\n|\u00afPx|B\u2020\u00afPx, B\u2020:= (B\u22a4B)\u22121B\u22a4,\nwhere B\u2020denotes the Moore-Penrose pseudoinverse. The\nfollowing result demonstrates the effectiveness of the robust\ncontrol.\nProposition 1: Using the control u=\u2212\u00afKx+udrenders\nthe system (1) asymptotically stable.\nProof: We follow the method in [18, Chapter 14.2],\nusing the same Lyapunov function V(x) =1\n2x\u22a4\u00afPx as\nbefore. The quadratic form Vis strictly positive definite\n(by assumption) hence is a valid Lyapunov function. Taking\nderivative along system trajectories,\n\u02d9V(x) =\u2212x\u22a4\u00afP(A\u2212B\u00afK)x+ (Bud+d)\u22a4\u00afPx\n\u2264 |d| \u00b7 |\u00afPx| \u2212\u03bb|\u00afPx|\n\u2264(|d| \u2212\u03bb)|\u00afPx|<0.\nFor the first inequality we recall that \u2212x\u22a4\u00afP(A\u2212B\u00afK)x\u22640\nand use Cauchy-Schwarz inequality. Next by properties of\nthe Moore-Penrose pseudoinverse, BB\u2020\u00afPxis the orthogo-\nnal projection of \u00afPxonto the column span of B. Hence,\nunder Assumption 2-(i), we have BB\u2020\u00afPx=\u00afPxtherefore\nBud=\u2212\u03bb\n|\u00afPx|\u00afPx.\nRemark 2: When implementing udwe add a regularizing\nparameter to avoid division by zero, which makes the system\nasymptotically stable till it enters a ball around the origin,\ni.e., practical stability.\nC. Data-driven (Simulator-based) implementation\nIn this section, we present a method to implement the\ncontrol u=\u2212\u00afKx+udwith only access to a disturbance-\nfree system simulator of (1). We use the dual ensemble\nKalman filter (dual EnKF) algorithm [15] for the same. To\nuse the algorithm, we need the following assumptions:\nAssumption 3: We have access to the following:\n\n--- Page 3 ---\n(i) Knowledge of optimal control matrices Q, R, G .\n(ii) Simulator of the dynamical system with no distur-\nbance, that is, we have access to function evaluations\nofS(x, u) :=Ax+Bu. Moreover, we assume we can\nrun the simulator backward in time, that is, to find\na trajectory of the system by specifying the terminal\ncondition.\nRemark 3: With access to a perfect simulator, one may\nexactly find the model matrices AandBinn+mevalua-\ntions of the simulator (set u= 0and evaluate the simulator\nat the basis vectors of Rnto find Aand equivalently for B).\nHowever, the utility of the simulator is revealed when we\nconsider the nonlinear case, especially for high-dimensional\nsystems like partial differential equations, where estimating\nthe state dynamics in this manner is not possible.\nThe emphasis is on obtaining the controller in a model-\nfree way. The following three steps are done (which are\nelaborated upon after listing them):\n1) Find an approximation to \u00afP, the solution of the ARE,\nusing the dual EnKF algorithm [15]. See Appendix I-A\nfor details\n2) Find an approximation \u00afu(N)for\u00afu:=\u2212\u00afKxusing [15,\nAlgorithm 2] (recalled in Appendix I-C)\n3) Find an approximation u(N)\ndforudusing Algorithm 1\nStep 1: Using the simulator, we find \u00afP(N), an approxi-\nmation to the solution of the ARE \u00afPby running the dual\nEnKF algorithm of [15], which simulates Ncopies of the\ndynamical system along with a mean-field coupling term\nto approximate the solution of the ARE. The algorithm is\nprovided in Appendix I-A.\nStep 2: To help evaluate the optimal control in a model-\nfree way, define the Hamiltonian,\nH(x, u) := ( \u00afP(N)x)\u22a4(Ax+Bu) +1\n2(x\u22a4Qx+u\u22a4Ru)\n= (\u00afP(N)x)\u22a4S(x, u) +1\n2L(x, u) (4)\nThe Hamiltonian is constructed so that it can be evaluated\nusing function calls of S. For a fixed x, it is a quadratic\nfunction of uwith the unique minima at the optimal control.\nTherefore, \u00afu= arg minuH(x, u)and the minimization can\nbe carried out using gradient estimation as shown in [15,\nAlgorithm 2] (recalled in Appendix I-C) or using zero-order\nmethods, such as [1].\nStep 3: Similarly, to find udwe solve ud=\n\u2212\u03bbarg minu|Bu\u2212\u00afPx\n|\u00afPx||in Algorithm 1. If Bis known, one\nmay directly use the Moore-Penrose pseudo inverse. If Bis\nunknown, one may use zero order optimization methods [1]\nwhere Bu=S(0, u)can be obtained using only access to\nsimulator or one may estimate Bas specified in Remark 3\nand use the pseudo inverse. Using zero order optimization\nis preferred over estimating Bin cases when the simulator\nis very high dimensional.Algorithm 1 Algorithm to find ud\nInput: System state x, regularizing parameter r, robust gain\n\u03bb,\u00afP(N)\n1:r1:= max( |\u00afP(N)x|, r)\n2:ifBis known then\n3: B\u2020:= (B\u22a4B)\u22121B\u22a4\n4: v(N):=r\u22121\n1B\u2020\u00afP(N)x\n5:else if Bis unknown then\n6: v(N):= arg minv|S(0, v)\u2212\u00afP(N)x\nr1|\n7:end if\n8:return u(N)\nd:=\u2212\u03bbv(N)\nIII. E XTENSION FOR NONLINEAR DISCRETIZED MODEL\nOF THE PDE\nLet us consider now the main result of this paper, dealing\nwith the case of the nonlinear affine in control system\n(1). We will extend the robust control design methodology\npresented in Section II to (1). Similar to the previous\nsection, we will obtain the robust control as a sum of\na stabilizing control and a robustification term, and then\npresent a simulator based methodology to obtain both terms.\nA. Stabilizing control\nIn an effort to find a stabilizing control \u00afu, we consider, for\nany arbitrary but fixed T >0, the optimal control problem\nmin\nu(\u00b7)\u0012\nG(x(T)) +1\n2ZT\n0c(x(t)) +u(t)\u22a4Ru(t)| {z }\n=:L(x(t),u(t))dt\u0013\n(5a)\ns.t. system (1) with zero disturbance, that is, d\u22610(5b)\nwhere c,Gare non-negative real valued function, and R\u2208\nRm\u00d7mis symmetric and strictly positive definite. The value\nfunction for the problem is defined as the cost-to-go,\n\u03d5(s, x) := min\nu(\u00b7)\u0012\nG(x(T)) +1\n2ZT\nsL(x(t), u(t))dt\u0013\ns.t. system (1) with Xs=xandd\u22610\nand it satisfies the Hamilton Jacobi Bellman partial differen-\ntial equation. The optimal control is computed as \u00afu(t, x) =\n\u2212R\u22121b(x)\u22a4\u2207\u03d5(t, x)[22].\nB. Robust control\nAssumption 4: (i) There exists a control law \u00afuwhich\nmakes (1) with zero disturbance asymptotically stable.\nMoreover, there exists a strictly positive definite Lya-\npunov function Vwith \u02d9V < 0along the controlled\ntrajectories with zero disturbance.\n(ii) The rank of b(x)isnfor all x\u2208Rn.\n(iii) There exists a known \u03bbsuch that 0\u2264 |d(t, x)|<\n\u03bb(t, x)<\u221efor each (t, x)\u2208[0,\u221e)\u00d7Rn.\nRemark 4: Assumption 4-(ii) is required for the the-\noretical result, but will be relaxed in the PDE control\nimplementation.\n\n--- Page 4 ---\nSimilar to the linear case, consider the controller u=\n\u00afu+udwith\nud:=\u2212\u03bb\n|\u2207V|b\u2020\u2207V, b\u2020:= (b\u22a4b)\u22121b\u22a4.\nProposition 2: Using the control u= \u00afu+udrenders the\nsystem (1) asymptotically stable.\nProof: We follow the method in [18, Chapter 14.2].\nTaking the derivative of Valong system trajectories,\n\u02d9V(x) =\u2207V(x(t))\u22a4\u0012\na(x(t)) +b(x(t))\u00afu(t)\n+b(x(t))ud(t) +d(t, x)\u0013\n\u2264(|d| \u00b7 |\u2207V(x)| \u2212\u03bb|\u2207V(x)|)\n\u2264(|d| \u2212\u03bb)|\u2207V(x)|<0.\nFor the first equality we recall Assumption 4 and Cauchy-\nSchwarz inequality. Moreover, under Assumption 4-(ii),\nbud=\u2212\u03bb\n|\u2207V(x)|\u2207V(x).\nC. Data-driven (Simulator-based) implementation\nTo approximate \u00afuandudwe use an approach similar\nin spirit to Section II. First, we use the nonlinear dual\nEnKF algorithm [15] to approximate the gradient of value\nfunction \u2207\u03d5(N)(x). Implementation details can be found in\nAppendix I-B. Then we define the nonlinear counterpart of\nthe Hamiltonian\nH(x, u) := (\u2207\u03d5(x)(N))\u22a4S(x, u) +1\n2L(x, u) (6)\nwhere the simulator is now nonlinear, that is, S(x, u) =\na(x)+b(x)u. The Hamiltonian can again be evaluated using\nfunction calls of the simulator. Moreover, it is quadratic in\nthe control, and can be minimized using [15, Algorithm\n2] (recalled in Appendix I-C), or zero order optimization\napproaches [1]. Similar to the linear control case, to find\nudwe use ud=\u2212\u03bbarg minu|b(x)u\u2212\u2207V(x)\n|\u2207V(x)||as given\nin Algorithm 1 (replacing \u00afP(N)xby\u2207V(N)(x)) where if\nbis not known, the optimization can be done by zero-order\napproaches [1] or by estimating bsimilar to Remark 3.\nIV. A PPLICATION TO FORCED NONLINEAR PDE S\nWe consider PDEs of the form\n\u2202z\n\u2202t(t, y) +F(z(t, y)) =\u03c9(t, y),\nwhere Fis a differential operator that specifies the structure\nof the PDE, zis the state of the PDE and \u03c9is the\nexternal input. The functions z, \u03c9 :R+\u00d7[0, L]\u2192R.\nMathematically, the goal of stabilization is to make\nlim\nt\u2192\u221e\u2225z(t,\u00b7)\u2225L2:= lim\nt\u2192\u221eZL\n0|z(t, y)|2dy= 0. (7)\nFor numerical implementation, we consider a time interval\nof[0, T]and discretize the PDE in space on a grid of p\nuniformly-spaced points in [0, L]to obtain zp(t)\u2208Rpfor\neacht\u22650, so that the PDE is reduced to a set of pODEs.We choose mbasis functions {\u03c7j}m\nj=1to discretize the\ncontrol as \u03c9(x, t) =Pm\nj=1\u03c7j(x)Uj(t)where \u03c7j: [0, L]\u2192\nRis the indicator function of [j\u22121\nm,j\nm]andUj: [0, T]\u2192R.\nThen U:= (U1, U2, . . . , U m)is interpreted as the control.\nThe discretized PDE has nonlinear affine in control form,\ndzp\ndt+F(zp) =BU (8)\nwhere Fapproximates the derivatives and Bobtained by\ndiscretizing {\u03c7j}m\nj=1. To study the effect of disturbances,\nwe let U(t) =u(t) +d(t)where u(t)is the control action\napplied to the system and Bd(t)is the effective disturbance\nacting on the system. We consider here disturbances added\ndirectly to the control, which is a special case of the\ntheory presented earlier. In the simulations, we consider\nonly time varying disturbances, so we denote disturbance\nasd(t). We use the Controlgym library in Python [31]\nfor numerical implementation. Now we first recall the heat\nequation and Burgers equations, give some information\nabout the implementation of both PDEs, then go on to\ndiscuss the simulation results.\nA. Heat equation\nThe heat equation is given by\n\u2202z\n\u2202t(t, y)\u2212\u03bd\u22022z\n\u2202y2(t, y) =\u03c9(t, y).\nThe value of \u03bd= 0.002 is used. Since the PDE is linear,\nupon discretization, (8) reduces to an LTI system, hence we\nuse the robust control method design discussed in Section\nII to implement the robust control. The dimension of the\ncontrol used is m= 10 , while the dimension of the\nstate is n= 100 , thus relaxing Assumption 2-(i) in the\nimplementation. Other simulations details can be obtained\nin Appendix II-A.\nB. Burgers\u2019 equation\nRecall the Burgers\u2019 equation,\n\u2202z\n\u2202t(t, y) +z(t, y)\u2202z\n\u2202y(t, y)\u2212\u03bd\u22022z\n\u2202y2(t, y) =\u03c9(t, y).\nWe present two sets of results to demonstrate the per-\nformance of the robust control algorithm developed in the\npaper when applied to the Burgers equation. In the first set\nof results, we calculate the robust control using a simulator\nof the linear reduced order model of the PDE, and in the\nsecond set of results, the robust control is calculated using\na simulator of the full nonlinear discretized PDE. In both\nresults, the obtained control is tested by applying it on the\nfull nonlinear discretized PDE. We study both sets of results\nfor two values of viscosity, \u03bd= 0.02,0.002.\nControl design using linear reduced model: We perform a\nmodel reduction on the Burgers\u2019 equation to obtain a linear\nsystem using the Dynamic Mode Decomposition for control\n(DMDc) algorithm [32]. The DMDc yields a projection\nmatrix \u03a6\u2208Rn\u00d7pwith orthogonal rows, where nis the\n\n--- Page 5 ---\n0.00.010.1d0d(t) =d0sin(t),\u03bd=0.002\nZero\ncontrol0 0.02 0.2\n\u03bb0.00.010.1d0d(t) =d0,\u03bd=0.002\n10\u2212610\u2212510\u2212410\u2212310\u2212210\u22121100(a)\n024/bardblz(t,\u00b7)/bardblL2d(t) =d0sin(t),\u03bd=0.002\n10\u2212210\u22121\n0.00 0.02 0.04 0.06 0.08 0.10\nTime(s)0510/bardblz(t,\u00b7)/bardblL2d(t) =d0,\u03bd=0.002\n10\u221213\u00d710\u22121uncontrolled\noptimal control\nrobust control\n(b)\nFig. 1: Results for control of heat equation. (a) Mean of\n\u2225z(T,\u00b7)\u2225L2\n\u2225z(0,\u00b7)\u2225L2over 100 different simulations (b) Mean and\nvariance of \u2225z(t,\u00b7)\u2225L2ford0= 0.1, \u03bb= 0.2.\ndimension of the reduced state (usually n\u226ap), and an LTI\nsystem\n\u02d9x=Ax+Bu (9)\nwhere x\u2208Rnis the reduced state and satisfies the relation\nx= \u03a6zp, and u\u2208Rmwhere as described, nandm\nare chosen by the user. The discretized PDE state can\nbe reconstructed as zp= \u03a6\u22a4x. DMDc yields a discrete-\ntime system, but we transform it into its continuous-time\nequivalent, since we design control in the continuous time\ndomain.\nUsing a simulator for the obtained linear system (9),\nwe get an approximation to the optimal stabilizing LQ\ncontroller, with the linear EnKF methodology described\nin Section II. That is, we choose optimal control weights\nQ, R, G (as described in (3)) and find \u00afP(N)for the LTI\nsystem (9). The stabilizing control is calculated as u=\u2212R\u22121B\u22a4\u00afPxwhere x= \u03a6zp. We use the robust control\nmethodology described in Section II to design ud.\nControl design using full nonlinear model: Using a simu-\nlator for the full nonlinear PDE (8), we obtain a stabilizing\ncontrol \u00afuand robust controller udwith the nonlinear dual\nEnKF methodology discussed in Section III. We choose\nc(x) := |x|2andG(x) =|x|2in (5). The dimension\nof the state used is p= 128 and the dimension of the\ncontrol is m= 10 , thus relaxing Assumption 4-(ii) in the\nimplementation.\nC. Discussion of results\nWe study the effect of two types of disturbances, sinu-\nsoidal with d(t) :=d0sin(t)and constant d(t) :=d0. The\n\u03bbfunction is chosen as a constant function. The obtained\ncontrol is applied to the full discretized nonlinear PDE (8).\nBoth PDEs are initialized for 100 randomly sampled iid\ninitial conditions (inspired from the previous work [32]):\nz(0, x) =\u03b1sech\u00121\n\u03b2\u0012\nx\u22121\n2L\u0013\u0013\n, L = 1,\n\u03b1\u223cunif(0 .9,1.1), \u03b2\u223cunif(0 .04,0.06).\nThe 100 trajectories are simulated for the case of zero\ncontrol (that means \u00afu=ud= 0), and then with the robust\ncontrol given by various values of \u03bb(the stabilizing control\n\u00afuis obtained from the dual EnKF). These simulations\nare repeated for various values of \u03bb,d0, both types of\ndisturbances mentioned, and the values of \u03bdmentioned.\nRecall that the control objective (7) in finite time is to\ndrive\u2225z(T,\u00b7)\u2225L2as close to zero as possible. To illustrate\nthe performance of the controllers we make two plots for\nboth PDEs. The first plot is a heat map depicting the\nmean value (over the 100 simulations) of\u2225z(T,\u00b7)\u2225L2\n\u2225z(0,\u00b7)\u2225L2\u2013 the\nratio is plotted to highlight the order of magnitude by\nwhich \u2225z(T,\u00b7)\u2225L2has reduced relative to its initial value\n\u2225z(0,\u00b7)\u2225L2. Moreover, in a second plot, we plot the mean\nand variance (over the 100 simulations) of \u2225z(t,\u00b7)\u2225L2as a\nfunction of tford0= 0.1and\u03bb= 0.2. The trajectory is\ngenerated using three different control policies \u2013 \u201cuncon-\ntrolled\u201d trajectories are when the control is zero, \u201coptimal\ncontrolled\u201d trajectories have \u03bb= 0(that is, only the optimal\nstabilizing control is applied and robust control is zero)\nand \u201crobust controlled\u201d trajectories are with \u03bb= 0.2. The\nplots for heat equation, Burgers with DMDc and Burgers\nwith full nonlinear model are found in Figures 1, 2, and\n3, respectively. Other simulations details can be obtained in\nAppendix II-B.\nFor both PDEs, we observe that in the presence of dis-\nturbances, the robust control works better than the optimal\ncontrol (control with \u03bb= 0) in stabilizing the PDE. For\nthe Burgers\u2019 equation in particular, the trajectories with\nrobust control exhibit an order of magnitude lower value\nof\u2225z(T,\u00b7)\u2225L2compared to those with only the optimal\ncontrol. Additionally, for the Burgers\u2019 equation, we also\nobserve that the optimal control obtained using the full\nnonlinear model is significantly more effective than using\n\n--- Page 6 ---\nthe reduced-order DMDc model \u2013 the controlled state settles\nmuch faster and closer to zero with the former than the latter.\nHowever, while the transient performance of the robust\ncontrol is much better using the full nonlinear model, the\nsettling performance is similar using both DMDc and the\nfull nonlinear model. Thus the robust control term also\ncompensates for model mismatch between DMDc and the\nfull nonlinear model. The conclusions presented are true for\nall values of \u03bbchosen, both types of disturbances, and both\nvalues of viscosity.\nV. F UTURE WORK\nSome avenues for future work are: (i) to extend this\napproach to different PDEs such as the Allen-Cahn equation\nor the Korteweg deVries equation, (ii) to design a controller\nusing information from sensors, thus turning a full state\nfeedback problem into a partially observed problem (iii) to\nextend to the case when dis Gaussian white noise.\nREFERENCES\n[1] F. B ACH AND V. P ERCHET ,Highly-smooth zero-th order online op-\ntimization , in 29th Annual Conference on Learning Theory, V . Feld-\nman, A. Rakhlin, and O. Shamir, eds., vol. 49 of Proceedings of\nMachine Learning Research, Columbia University, New York, New\nYork, USA, 23\u201326 Jun 2016, PMLR, pp. 257\u2013283.\n[2] A. B ARBAGALLO , D. S IPP,AND P. J. S CHMID ,Closed-loop control\nof an open cavity flow using reduced-order models , Journal of Fluid\nMechanics, 641 (2009), pp. 1\u201350.\n[3] K. B HATTACHARYA , B. H OSSEINI , N. B. K OVACHKI ,AND A. M.\nSTUART ,Model reduction and neural networks for parametric PDEs ,\nThe SMAI Journal of Computational Mathematics, 7 (2021), pp. 121\u2013\n157.\n[4] A. N. B ISHOP AND P. D ELMORAL ,On the mathematical theory\nof ensemble (linear-gaussian) kalman\u2013bucy filtering , Mathematics of\nControl, Signals, and Systems, 35 (2023), pp. 835\u2013903.\n[5] A. B. B LANCHARD , G. Y. C ORNEJO MACEDA , D. F AN, Y. L I,\nY. Z HOU , B. R. N OACK ,AND T. P. S APSIS ,Bayesian optimization\nfor active flow control , Acta Mechanica Sinica, (2021), pp. 1\u201313.\n[6] T. D URIEZ , S. L. B RUNTON ,AND B. R. N OACK ,Machine Learn-\ning Control-Taming Nonlinear Dynamics and Turbulence , vol. 116,\nSpringer, 2017.\n[7] G. E VENSEN ,Data Assimilation. The Ensemble Kalman Filter ,\nSpringer-Verlag, New York, 2006.\n[8] D. F AN, L. Y ANG , Z. W ANG , M. S. T RIANTAFYLLOU ,AND G. E.\nKARNIADAKIS ,Reinforcement learning for bluff body active flow\ncontrol in experiments and simulations , Proceedings of the National\nAcademy of Sciences, 117 (2020), pp. 26091\u201326098.\n[9] M. F AZEL , R. G E, S. K AKADE ,AND M. M ESBAHI ,Global conver-\ngence of policy gradient methods for the linear quadratic regulator ,\nin Proceedings of the 35th International Conference on Machine\nLearning, J. Dy and A. Krause, eds., vol. 80 of Proceedings of\nMachine Learning Research, PMLR, 10\u201315 Jul 2018, pp. 1467\u20131476.\n[10] W. H. F LEMING AND S. K. M ITTER ,Optimal Control and Nonlin-\near Filtering for Nondegenerate Diffusion Processes , Stochastics, 8\n(1982), pp. 63\u201377.\n[11] S. F RESCA , L. D EDE\u2019,AND A. M ANZONI ,A comprehensive deep\nlearning-based approach to reduced order modeling of nonlinear\ntime-dependent parametrized PDEs , Journal of Scientific Computing,\n87 (2021), pp. 1\u201336.\n[12] P. G ARNIER , J. V IQUERAT , J. R ABAULT , A. L ARCHER ,\nA. K UHNLE ,AND E. H ACHEM ,A review on deep reinforcement\nlearning for fluid mechanics , Computers & Fluids, 225 (2021),\np. 104973.\n[13] C. H OFFMANN AND P. R OSTALSKI ,Linear optimal control on factor\ngraphs \u2014 a message passing perspective \u2014 , IFAC-PapersOnLine, 50\n(2017), pp. 6314\u20136319. 20th IFAC World Congress.\n[14] S. H OVLAND , J. T. G RAVDAHL ,AND K. E. W ILLCOX ,Explicit\nmodel predictive control for large-scale systems via model reduction ,\nJournal of guidance, control, and dynamics, 31 (2008), pp. 918\u2013926.[15] A. A. J OSHI , A. T AGHVAEI , P. G. M EHTA ,AND S. P. M EYN,Con-\ntrolled interacting particle algorithms for simulation-based reinforce-\nment learning , Systems & Control Letters, 170 (2022), p. 105392.\n[16] E. K AISER , J. N. K UTZ,AND S. L. B RUNTON ,Data-driven dis-\ncovery of Koopman eigenfunctions for control , Machine Learning:\nScience and Technology, 2 (2021), p. 035023.\n[17] H. J. K APPEN ,Linear theory for control of nonlinear stochastic\nsystems , Phys. Rev. Lett., 95 (2005), p. 200201.\n[18] H. K. K HALIL ,Nonlinear systems , Macmillan Pub. Co., New York,\n1992.\n[19] H. K WAKERNAAK AND R. S IVAN ,Linear optimal control systems ,\nWiley Interscience, New York, 1972.\n[20] F. L EIBFRITZ AND S. V OLKWEIN ,Numerical feedback controller\ndesign for PDE systems using model reduction: Techniques and case\nstudies , in Real-Time PDE-Constrained Optimization, SIAM, 2007,\npp. 53\u201372.\n[21] S. L EVINE ,Reinforcement learning and control as probabilistic\ninference: Tutorial and review , 2018.\n[22] D. L IBERZON ,Calculus of variations and optimal control theory ,\nPrinceton University Press, Princeton, NJ, 2012.\n[23] H. M OHAMMADI , A. Z ARE, M. S OLTANOLKOTABI ,AND M. R.\nJOVANOVI \u00b4C,Convergence and sample complexity of gradient meth-\nods for the model-free linear\u2013quadratic regulator problem , IEEE\nTransactions on Automatic Control, 67 (2022), pp. 2435\u20132450.\n[24] J. L. P ROCTOR , S. L. B RUNTON ,AND J. N. K UTZ,Dynamic mode\ndecomposition with control , SIAM Journal on Applied Dynamical\nSystems, 15 (2016), pp. 142\u2013161.\n[25] D. S IPP AND P. J. S CHMID ,Linear closed-loop control of fluid\ninstabilities and noise-induced perturbations: A review of approaches\nand tools , Applied Mechanics Reviews, 68 (2016), p. 020801.\n[26] A. T AGHVAEI , J.DEWILJES , P. G. M EHTA ,AND S. R EICH ,Kalman\nfilter and its modern extensions for the continuous-time nonlinear\nfiltering problem , Journal of Dynamic Systems, Measurement, and\nControl, 140 (2017), p. 030904.\n[27] E. T ODOROV ,Linearly-solvable markov decision problems , in Ad-\nvances in Neural Information Processing Systems, B. Sch \u00a8olkopf,\nJ. Platt, and T. Hoffman, eds., vol. 19, MIT Press, 2007.\n[28] A. T SOLOVIKOS , E. B AKOLAS , S. S URYANARAYANAN ,AND\nD. G OLDSTEIN ,Estimation and control of fluid flows using sparsity-\npromoting dynamic mode decomposition , IEEE Control Systems\nLetters, 5 (2020), pp. 1145\u20131150.\n[29] S. V IJAYAKUMAR , K. R AWLIK ,AND M. T OUSSAINT ,On stochastic\noptimal control and reinforcement learning by approximate inference ,\nin Robotics: Science and Systems VIII, N. Roy, P. Newman, and\nS. Srinivasa, eds., MIT Press, 2013, pp. 353\u2013360.\n[30] T. Y ANG , R. S. L AUGESEN , P. G. M EHTA ,AND S. P. M EYN,\nMultivariable feedback particle filter , Automatica, 71 (2016), pp. 10\u2013\n23.\n[31] X. Z HANG , W. M AO, S. M OWLAVI , M. B ENOSMAN ,AND\nT. B AS\u00b8AR,Controlgym: Large-scale safety-critical control environ-\nments for benchmarking reinforcement learning algorithms , arXiv\npreprint arXiv:2311.18736, (2024).\n[32] X. Z HANG , S. M OWLAVI , M. B ENOSMAN ,AND T. B AS\u00b8AR,Policy\noptimization for pde control with a warm start , arXiv preprint arXiv\n2403.01005, (2024).\nAPPENDIX I\nDUAL ENKF ALGORITHM\nIn this algorithm, we simulate over the time horizon [0, T]\nan ensemble of Nparticles {Yi\nt: 1\u2264i\u2264N, 0\u2264t\u2264T}\nwhere the evolution equation for each particle is given by\nan Ito stochastic differential equation (SDE).\nA. Linear system\nThe SDE [15, Section 2] is given by\ndYi\nt=AYi\ntdt+Bd\u2190\u03b7i\nt+L(N)\nt \nCYi\nt+C\u02c6n(N)\nt\n2!\n,\nYi\nTi.i.d\u223c N (0, ST),1\u2264i\u2264N,\n\n--- Page 7 ---\n0.00.010.11.0d0d(t) =d0sin(t),\u03bd=0.02 d(t) =d0sin(t),\u03bd=0.002\nZero\ncontrol00.02 0.2 220\n\u03bb0.00.010.11.0d0d(t) =d0,\u03bd=0.02\nZero\ncontrol00.02 0.2 220\n\u03bbd(t) =d0,\u03bd=0.002\n10\u2212510\u2212410\u2212310\u2212210\u22121100(a) Mean of\u2225z(T,\u00b7)\u2225L2\n\u2225z(0,\u00b7)\u2225L2over 100 different simulations\n01234/bardblz(t,\u00b7)/bardblL2d(t) =d0sin(t),\u03bd=0.02\n10\u2212310\u2212210\u22121uncontrolled optimal control robust control\n01234d(t) =d0sin(t),\u03bd=0.002\n10\u2212210\u22121\n0 1 2 3\nTime(s)024/bardblz(t,\u00b7)/bardblL2d(t) =d0,\u03bd=0.02\n10\u2212310\u2212210\u22121\n0 1 2 3\nTime(s)024d(t) =d0,\u03bd=0.002\n10\u2212210\u22121 (b) Mean and variance of \u2225z(t,\u00b7)\u2225L2ford0= 0.1, \u03bb= 0.2.\nFig. 2: Results for control of Burgers equation using reduced order DMDc model.\n0.00.010.11.0d0d(t) =d0sin(t),\u03bd=0.02 d(t) =d0sin(t),\u03bd=0.002\nZero\ncontrol00.02 0.2 220\n\u03bb0.00.010.11.0d0d(t) =d0,\u03bd=0.02\nZero\ncontrol00.02 0.2 220\n\u03bbd(t) =d0,\u03bd=0.002\n10\u2212610\u2212510\u2212410\u2212310\u2212210\u22121100\n(a) Mean of\u2225z(T,\u00b7)\u2225L2\n\u2225z(0,\u00b7)\u2225L2over 100 different simulations\n01234/bardblz(t,\u00b7)/bardblL2d(t) =d0sin(t),\u03bd=0.02\n10\u2212310\u22122uncontrolled optimal control robust control\n01234d(t) =d0sin(t),\u03bd=0.002\n10\u2212310\u22122\n0 1 2 3\nTime(s)024/bardblz(t,\u00b7)/bardblL2d(t) =d0,\u03bd=0.02\n10\u2212310\u22122\n0 1 2 3\nTime(s)024d(t) =d0,\u03bd=0.002\n10\u2212310\u22122 (b) Mean and variance of \u2225z(t,\u00b7)\u2225L2ford0= 0.1, \u03bb= 0.2.\nFig. 3: Results for control of Burgers equation using full nonlinear model.\n\u03b7iare an i.i.d Brownian motions with covariance R\u22121and\n\u02c6n(N)\nt:=\u0010\n1\nNPN\nj=1Yj\nt\u0011\n, and L(N)\nt:=S(N)\ntC\u22a4with\nS(N)\nt:=1\nNNX\nj=1(Yj\nt\u2212\u02c6n(N)\nt)(Yj\nt\u2212\u02c6n(N)\nt)\u22a4.\nFinally, \u00afP(N):= (S(N)\n0)\u22121.B. Nonlinear system\nThe SDE [15, Section 3] for the particle system is\ndYi\nt=a(Yi\nt)dt+b(Yi\nt)d\u2190\u03b7i\nt\n+\uf8eb\n\uf8edNX\nj=1(Yj\nt\u2212n(N)\nt)(c(Yj\nt)\u2212\u02c6c(N)\nt)\u22a4\uf8f6\n\uf8f8(c(z) + \u02c6c(N))\nN\u22121\nYi\nTi.i.d\u223c N (0, ST),1\u2264i\u2264N,\nand\u02c6c(N)\nt :=N\u22121PN\ni=1c(Yi\nt). Finally, \u2207\u03d5(N)(x) :=\n(S(N)\n0)\u22121x, where S(N)\nt is the empirical covariance of\n{Yi\nt}, defined same as in the linear case.\n\n--- Page 8 ---\nC. Algorithm for Hamiltonian minimization\nAlgorithm 2 minimizes Hamiltonian and calculates \u00afu\nfor Section II. To minimize Hamiltonian in Section III, in\nAlgorithm 2 make the following two changes: replace \u00afP(N)\nby\u2207\u03d5(N)and use the Hamiltonian defined in (6).\nAlgorithm 2 Algorithm for Hamiltonian minimization\nInput: System state x,\u00afP(N),Q, R , Hamiltonian definition\nfrom (4), {ei}m\ni=1the standard basis of Rm\n1:ifBis known then\n2: return \u00afu(N):=\u2212R\u22121B\u22a4\u00afP(N)x\n3:else if Bis unknown then\n4: fori=1,2,. . . ,m do\n5: (\u00afu(N))i:=H(x, R\u22121ei)\u2212H(x,0)\u22121\n2(R\u22121)ii\n6: end for\n7:end if\nAPPENDIX II\nSIMULATION DETAILS\nA. Heat equation\nSimulation parameters are as follows. The simulation\ntime T= 0.1with simulation time step = 0.001. The\nnumber of states of the discretized PDE (8) is p= 100 . The\nnumber of control basis functions is m= 8, and they are \u03c7j\nis the indicator functions of [i\n10,(i+1)\n10]. The regularization\nparameter for robust control is r= 0.002. The matrices\nQ=I,G=I,R=Ifor both, the full nonlinear control\nand control using DMDc model. The number of dual EnKF\nparticles is chosen as N= 10000 . The controlgym library\n[31] is used for PDE simulation.\nB. Burgers equation\nSimulation parameters are as follows. The simulation\ntimeT= 3with simulation time step = 0.001. The number\nof states of the discretized PDE (8) is p= 128 . The number\nof control basis functions is m= 10 , and they are \u03c7j\nis the indicator functions of [i\n10,(i+1)\n10]. The regularization\nparameter for robust control is r= 0.002. The number of\nstates in the DMDc model (9) is n= 10 . The matrices\nQ=I,G=I,R= 0.1Ifor both, the full nonlinear\ncontrol and control using DMDc model. The number of dual\nEnKF particles is chosen as N= 1000 . The controlgym\nlibrary [31] is used for PDE simulation.",
  "project_dir": "artifacts/projects/Dual Ensemble Kalman Filter for Robust Control of Nonlinear Systems",
  "communication_dir": "artifacts/projects/Dual Ensemble Kalman Filter for Robust Control of Nonlinear Systems/.agent_comm",
  "assigned_at": "2025-09-03T20:49:54.421458",
  "status": "assigned"
}